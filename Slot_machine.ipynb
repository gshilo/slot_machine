{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM7NCS6mPpU6"
      },
      "source": [
        "\n",
        "# **Slot Machine**<br>\n",
        "If you are using BigQuery with the on demand billing model where you are billed by bytes scanned, it may be a good idea to switch to the Editions model where you are billed for slots you used.\n",
        "\n",
        "There are Two quite complicated querstions though:\n",
        "\n",
        "\n",
        "1.   Will moving to Editions save me money ?\n",
        "2.   How many slots should I reserve, what is the optimal max_xlots value ?\n",
        "\n",
        "There is no way to be 100% accurate, but this set of queries aims to reduce the guesswork and guide you in finding better answers to the Two questions above.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-TZBaww_pFwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wOvhZiARoLC"
      },
      "source": [
        "**Usage**<br>\n",
        "You should run this in the project you want to test.\n",
        "The process creates several tables and views, if any of them already exists then it will stop with error, since we don't want to accidently delete user's important tables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_LhcAbq4NCG"
      },
      "source": [
        "**Step 1**<br>\n",
        "First we set some parameters such as region, timeframe and BigQuery edition. We also detect the current project id.\n",
        "\n",
        "Here is a short description of the parameters:\n",
        "\n",
        "- **region**: The region where your dataset is located. INFORMATION_SCHEMA is a regional data source.\n",
        "- **dataset**: The name of a new dataset where you want the tables and views related to slot machine to be created. Don't use an existing dataset.\n",
        "- **start and end timestamp**: We analyze the behavior in a limited timeframe. It should be long enough to contain all regular activity so it should contain at lease few days.\n",
        "- **verbose**: If true, Slot machine will print out the queries it runs plus additional interim data. If false it will operate quietly without printing out every query and interim result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQumnhKuKTDy"
      },
      "outputs": [],
      "source": [
        "import google.auth\n",
        "project_id = google.auth.default()[1]\n",
        "region = \"US\" # @param {\"type\":\"string\", \"placeholder\": \"Enter dataset region\"}\n",
        "dataset = \"test2\" # @param {\"type\":\"string\", \"placeholder\": \"Enter the target dataset where objects will be created\"}\n",
        "start_timestamp = \"2024-12-01\" # @param {\"type\":\"date\"}\n",
        "end_timestamp = \"2024-12-08\" # @param {\"type\":\"date\"}\n",
        "verbose = False # @param {\"type\":\"boolean\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9McWPU9c4gC2"
      },
      "source": [
        "**Step 2**<br>\n",
        "Here we import some python packages and create the dataset if it does not already exist."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HEnFFD1o7CJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import pandas_gbq\n",
        "from google.cloud import bigquery\n",
        "from google.cloud.exceptions import NotFound\n",
        "\n",
        "client = bigquery.Client()\n",
        "my_dataset = bigquery.Dataset(project_id+\".\"+dataset)\n",
        "my_dataset.location = region\n",
        "se_price = 0.04\n",
        "ee_price = 0.06\n",
        "eep_price = 0.1\n",
        "\n",
        "\n",
        "try:\n",
        "    client.get_dataset(project_id+\".\"+dataset)\n",
        "    print(\"Dataset {} already exists\".format(dataset))\n",
        "except NotFound:\n",
        "    print(\"Dataset {} is not found\".format(dataset), \"creating it.\")\n",
        "    dataset_object = client.create_dataset(my_dataset, timeout=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGfdPMkHGSA2"
      },
      "source": [
        "**Step 3**<br>\n",
        "Ceate a table with the on demand consumption for reference. The table name is bytes.<br>\n",
        "We select from INFORMATION_SCHEMA.JOBS to find the total bytes billed during the timeframe and what was the cost in USD.<br>Write down the result so you can compare it to the spend when using slots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtC29YYAvYJK"
      },
      "outputs": [],
      "source": [
        "print(dataset)\n",
        "query = \"create table if not exists \"+dataset+\".bytes as \"+\"\"\"SELECT\n",
        "  SUM(total_bytes_billed/1024/1024/1024/1024) AS total_tb_billed,\n",
        "  SUM(total_bytes_billed/1024/1024/1024/1024)*6.25 as cost_usd\n",
        "FROM \"\"\"+project_id+\".region-\"+region+\"\"\".INFORMATION_SCHEMA.JOBS\n",
        "WHERE\n",
        "  creation_time BETWEEN CAST(\\\"\"\"\"+start_timestamp+\"\"\"\\\" AS TIMESTAMP)\n",
        "  AND \"\"\"+\"CAST(\\\"\"+end_timestamp+\"\\\" AS TIMESTAMP)\"\n",
        "\n",
        "if verbose:\n",
        "  print(query)\n",
        "\n",
        "df = pandas_gbq.read_gbq(query, project_id=project_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eg60a_RPoR6b"
      },
      "source": [
        "**Step 4**<br>\n",
        "Then we query INFORMATION_SCHEMA.JOBS_TIMELINE on the same time range to see how many slots were used in every second of the time range. We send the result to a table called slots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKCiC-HYdD0d"
      },
      "outputs": [],
      "source": [
        "query = \"create table if not exists \"+dataset+\".slots as \"+\"\"\"SELECT\n",
        "  period_start,\n",
        "  count (distinct job_id) as total_queries,\n",
        "  SUM(period_slot_ms/1000) AS total_slot_ms,\n",
        "FROM\n",
        "  \"\"\"+project_id+\".region-\"+region+\"\"\".INFORMATION_SCHEMA.JOBS_TIMELINE\n",
        "WHERE\n",
        "  period_start BETWEEN CAST(\\\"\"\"\"+start_timestamp+\"\"\"\\\" AS TIMESTAMP)\n",
        "  AND \"\"\"+\"CAST(\\\"\"+end_timestamp+\"\\\" AS TIMESTAMP)\"\"\"\n",
        "\n",
        "query = query + \"\"\"\n",
        "GROUP BY\n",
        "  period_start\n",
        "ORDER BY\n",
        "  period_start DESC\"\"\"\n",
        "\n",
        "if verbose:\n",
        "  print(query)\n",
        "df = pandas_gbq.read_gbq(query, project_id=project_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qm_kRsP5FOxX"
      },
      "source": [
        "**Step 5**\n",
        "\n",
        "Create jobs table that later will be used to chart the number of jobs that ran in each time period along with the foregraoud/background jobs breakdown."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlxh9_CTE6JZ"
      },
      "outputs": [],
      "source": [
        "query = \"create table if not exists \"+dataset+\"\"\".jobs as SELECT\n",
        "  timeline.job_id,\n",
        "  timestamp_trunc(timeline.period_start, MINUTE) period_start,\n",
        "  jobs.user_email,\n",
        "  jobs.Job_type,\n",
        "  CASE\n",
        "    WHEN CONTAINS_SUBSTR(jobs.user_email, 'gserviceaccount') THEN 'background'\n",
        "    ELSE 'foreground'\n",
        "END\n",
        "  AS query_type\n",
        "FROM\n",
        "    `\"\"\"+project_id+\".region-\"+region+\"\"\".INFORMATION_SCHEMA.JOBS_TIMELINE` AS timeline\n",
        "  JOIN\n",
        "    `\"\"\"+project_id+\".region-\"+region+\"\"\".INFORMATION_SCHEMA.JOBS` AS jobs\n",
        "  ON\n",
        "    timeline.job_id = jobs.job_id\n",
        "WHERE\n",
        "  timeline.period_start BETWEEN CAST(\"\"\"+\"\\\"\"+start_timestamp+\"\\\"\"+\"\"\" AS TIMESTAMP)\n",
        "  AND CAST(\"\"\"+\"\\\"\"+end_timestamp+\"\\\"\"+\" AS TIMESTAMP)\"\n",
        "\n",
        "if verbose:\n",
        "  print(query)\n",
        "df = pandas_gbq.read_gbq(query, project_id=project_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyK-sFMX5sPj"
      },
      "source": [
        "**Step 6**<br>\n",
        "We create a dynamic query that creates buckets incremented by 50 from 0 to the highest number of slots used during the selected time period.<br>\n",
        "The original time periods are seconds, so we aggregate the data in Minute granularity and for each minute we take the high<br> watermark of slot usage (since BigQuery autoscaler's minimum is 1 minute).<br><br>\n",
        "Then we assign each minute to a specific bucket according to the maximum slots that ut used.<br>\n",
        "We create a view called bucketed to hold the result and enable further calculations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hR7o0v4Hd0f3"
      },
      "outputs": [],
      "source": [
        "query1 = \"select max(total_slot_ms) as max_slots from \"+dataset+\".slots\"\n",
        "df = pandas_gbq.read_gbq(query1, project_id=project_id)\n",
        "max_slots = int(df._get_value(0, 'max_slots'))\n",
        "query2 = \"create view if not exists \"+dataset+\".bucketed\"+\"\"\" as SELECT\n",
        "  timestamp_trunc(period_start, MINUTE) as period_start,\n",
        "  sum(total_queries) as total_queries,\n",
        "  max(case\n",
        "when total_slot_ms = 0 then 0\\n\"\"\"\n",
        "for i in range(0, max_slots, 50):\n",
        "  line = \"when total_slot_ms between \"+str(i)+\" and \"+str(i+50)+\" then \"+str(i+50)+'\\n'\n",
        "  query2 = query2 + line\n",
        "query2 = query2 + \"\"\"  end) as bucket\n",
        "FROM \"\"\"+dataset+\"\"\".slots\n",
        "  group by timestamp_trunc(period_start, MINUTE)\"\"\"\n",
        "\n",
        "if verbose:\n",
        "  print(query2)\n",
        "\n",
        "df = pandas_gbq.read_gbq(query2, project_id=project_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hB8ZVTWwAV7g"
      },
      "source": [
        "**Step 7**<br>\n",
        "Here we create a view called buckets_count that shows hoe many time periods (minutes) fall into each bucket.<br>\n",
        "Then we print the contents of the view."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvRkZok7eHMx"
      },
      "outputs": [],
      "source": [
        "query = \"create view if not exists \"+dataset+\"\"\".buckets_count as SELECT\n",
        "  bucket,\n",
        "  COUNT(*) as periods,\n",
        "FROM `\"\"\"+project_id+\".\"+dataset+\"\"\".bucketed`\n",
        "where bucket is not null\n",
        "GROUP BY\n",
        "  bucket\"\"\"\n",
        "\n",
        "if verbose:\n",
        "  print(query)\n",
        "\n",
        "df = pandas_gbq.read_gbq(query, project_id=project_id)\n",
        "query = 'select * from '+dataset+\".buckets_count order by bucket\"\n",
        "\n",
        "if verbose:\n",
        "  print(query)\n",
        "df = pandas_gbq.read_gbq(query, project_id=project_id)\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YI_O3n66xCKa"
      },
      "source": [
        "**Step 8**\n",
        "\n",
        "INFORMATION_SCHEMA.JOBS_TIMELINE and INFORMATION_SCHEMA.JOBS contain rows from time periods when we had BigQuery activity. Any time when no queries ran at all will not be represented there and this may distort the results (The price will not change but the charts will look different).\n",
        "So we want to create another table/view that will contain also all the \"silent\" times with a count of zero queries.\n",
        "We iterate over all the minutes from start_date to end_date and if this specific minute is not present in the slots table then we add it to an intermediate table \"silent_periods\".\n",
        "Once finished, we create a view called \"slots_full\" that contains both the silent times and the used times.\n",
        "\n",
        "\n",
        "We do the same with jobs table for jobs table, where we create views job_types and query_types adding zero records when there was no activity.\n",
        "\n",
        "Those \"padded\" views will be used later when we create charts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXU1M-iYf0gR"
      },
      "outputs": [],
      "source": [
        "periods = []\n",
        "slots = []\n",
        "queries = []\n",
        "\n",
        "query = \"select period_start from \"+dataset+\".slots\"\n",
        "df = pandas_gbq.read_gbq(query, project_id=project_id)\n",
        "\n",
        "minutes = pd.date_range(start =start_timestamp,\n",
        "         end =end_timestamp, freq ='min')\n",
        "\n",
        "for minute in minutes:\n",
        "  if minute not in df['period_start'].values:\n",
        "      periods.append(minute)\n",
        "      slots.append(0)\n",
        "      queries.append(0)\n",
        "\n",
        "new_df = pd.DataFrame({\n",
        "    \"period_start\": periods,\n",
        "    \"total_queries\": queries,\n",
        "    \"total_slot_ms\": slots,\n",
        "})\n",
        "\n",
        "pandas_gbq.to_gbq(new_df, dataset+\".silent_periods\", project_id=project_id, if_exists='replace',)\n",
        "\n",
        "query = \"\"\"CREATE VIEW if not exists\n",
        "  \"\"\"+dataset+\"\"\".slots_full AS\n",
        "SELECT\n",
        "  timestamp(period_start) as period_start, cast(total_queries as int) as total_queries, cast(total_slot_ms as int) as total_slot_ms\n",
        "FROM\n",
        "  \"\"\"+dataset+\"\"\".slots\n",
        "UNION ALL\n",
        "SELECT\n",
        "  timestamp(period_start) as period_start, cast(total_queries as int) as total_queries, cast(total_slot_ms as int) as total_slot_ms\n",
        "FROM\n",
        "  \"\"\"+dataset+\".silent_periods\"\n",
        "\n",
        "if verbose:\n",
        "  print(query)\n",
        "\n",
        "df = pandas_gbq.read_gbq(query, project_id=project_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWYjkYTMFv3m"
      },
      "outputs": [],
      "source": [
        "query = \"\"\"SELECT\n",
        "  TIMESTAMP_TRUNC(period_start, HOUR) AS hour,\n",
        "  SUM(foreground) AS fg_count,\n",
        "  SUM(background) AS bg_count\n",
        "FROM (\n",
        "  SELECT\n",
        "    period_start,\n",
        "    CASE query_type\n",
        "      WHEN 'foreground' THEN 1\n",
        "      ELSE 0\n",
        "  END\n",
        "    AS foreground,\n",
        "    CASE query_type\n",
        "      WHEN 'background' THEN 1\n",
        "      ELSE 0\n",
        "  END\n",
        "    AS background\n",
        "  FROM\n",
        "    \"\"\"+dataset+\"\"\".jobs)\n",
        "GROUP BY\n",
        "  hour\n",
        "ORDER BY\n",
        "  hour\"\"\"\n",
        "\n",
        "if verbose:\n",
        "  print(query)\n",
        "\n",
        "df = pandas_gbq.read_gbq(query, project_id=project_id)\n",
        "\n",
        "# Adding \"empty\" hours\n",
        "hours = pd.date_range(start =start_timestamp,\n",
        "         end =end_timestamp, freq ='h')\n",
        "periods = []\n",
        "fg_count = []\n",
        "bg_count = []\n",
        "for hour in hours:\n",
        "  if hour not in df['hour'].values:\n",
        "      periods.append(hour)\n",
        "      fg_count.append(0)\n",
        "      bg_count.append(0)\n",
        "\n",
        "new_df = pd.DataFrame({\n",
        "    \"hour\": periods,\n",
        "    \"fg_count\": fg_count,\n",
        "    \"bg_count\": bg_count,\n",
        "})\n",
        "\n",
        "df = pd.concat([df, new_df])\n",
        "pandas_gbq.to_gbq(df, dataset+\".query_types\", project_id=project_id, if_exists='replace',)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kq8y58bGLRmR"
      },
      "outputs": [],
      "source": [
        "query = \"\"\"select\n",
        "  TIMESTAMP_TRUNC(period_start, HOUR) AS hour,\n",
        "  SUM(query) AS query,\n",
        "  SUM(load_) AS load,\n",
        "  sum(extract_) as extract_,\n",
        "  sum(null_) as null_\n",
        "FROM (\n",
        "  SELECT\n",
        "    period_start,\n",
        "    CASE job_type\n",
        "      WHEN 'QUERY' THEN 1\n",
        "      ELSE 0\n",
        "    END\n",
        "    AS query,\n",
        "    CASE job_type\n",
        "      WHEN 'LOAD' THEN 1\n",
        "      ELSE 0\n",
        "    END AS load_,\n",
        "    CASE job_type\n",
        "      WHEN 'EXTRACT' THEN 1\n",
        "      ELSE 0\n",
        "    END AS extract_,\n",
        "    CASE job_type\n",
        "      WHEN 'COPY' THEN 1\n",
        "      ELSE 0\n",
        "    END AS copy,\n",
        "    CASE job_type\n",
        "      WHEN 'NULL' THEN 1\n",
        "      ELSE 0\n",
        "    END AS null_\n",
        "  FROM \"\"\" +dataset+\"\"\".jobs)\n",
        "GROUP BY\n",
        "  hour\n",
        "ORDER BY\n",
        "  hour\"\"\"\n",
        "\n",
        "if verbose:\n",
        "   print(query)\n",
        "\n",
        "df = pandas_gbq.read_gbq(query, project_id=project_id)\n",
        "\n",
        "# Adding \"empty\" hours\n",
        "hours = pd.date_range(start =start_timestamp,\n",
        "         end =end_timestamp, freq ='h')\n",
        "periods = []\n",
        "query = []\n",
        "load = []\n",
        "extract_ = []\n",
        "null_ = []\n",
        "for hour in hours:\n",
        "  if hour not in df['hour'].values:\n",
        "      periods.append(hour)\n",
        "      query.append(0)\n",
        "      load.append(0)\n",
        "      extract_.append(0)\n",
        "      null_.append(0)\n",
        "\n",
        "new_df = pd.DataFrame({\n",
        "    \"hour\": periods,\n",
        "    \"query\": query,\n",
        "    \"load\": load,\n",
        "    \"extract_\": extract_,\n",
        "    \"null_\": null_,\n",
        "})\n",
        "\n",
        "df2 = pd.concat([df, new_df])\n",
        "pandas_gbq.to_gbq(df2, dataset+\".job_types\", project_id=project_id, if_exists='replace',)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdAz0WMoChFM"
      },
      "source": [
        "For our next calculations we need to know Two numbers:\n",
        "\n",
        "\n",
        "1.   How many hours there are in our timeframe\n",
        "2.   How many time periods we have\n",
        "\n",
        "So steps 9 and 10 find those values.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uTDf2dtBhq0"
      },
      "source": [
        "**Step 9**<br>\n",
        "Calculate how many hours we have in the chosen time frame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znsJmnaRi_OS"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "date_format = '%Y-%m-%d'\n",
        "diff = datetime.strptime(end_timestamp,date_format) - datetime.strptime(start_timestamp, date_format)\n",
        "hours = diff.days * 24 + diff.seconds / 3600\n",
        "if verbose:\n",
        "  print(\"hours: \"+str(hours))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68WBq6DnDAGK"
      },
      "source": [
        "**Step 10**<br>\n",
        "Calculate how many time slots we have."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4UOfEKHC1Ha"
      },
      "outputs": [],
      "source": [
        "query = \"select count(*) as periods from \"+dataset+\".bucketed\"\n",
        "df = pd.io.gbq.read_gbq(query=query, project_id=project_id, dialect='standard')\n",
        "periods = df._get_value(0, 'periods')\n",
        "if verbose:\n",
        "  print(\"periods: \"+str(periods))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IerxqG84D_ji"
      },
      "source": [
        "**Step 11**<br>\n",
        "This is the heart of our calculation.<br><br>\n",
        "We know how many hours were in the selected timeframe and how many time periods we had during this timeframe.\n",
        "So we can calculate what is the percentage of all the time that we used each bucket.<br>\n",
        "If we know the percentage of time and we know how many wours we had in the timeframe then we can calculate how many hours each bucket was active.<br>\n",
        "And we know how much each slot/hour costs for each BigQuery edition so we can calculate the cost each bucket incured.\n",
        "\n",
        "So here we create a view called calculated that holds the bucket, the percentage of all time that this bucket was used, how many hours it was used, and how much we would be charged for it if we use the selected edition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpMDkjb3bqIo"
      },
      "outputs": [],
      "source": [
        "query = \"create view if not exists \"+dataset+\"\"\".calculated as SELECT\n",
        "  bucket,\n",
        "  round(periods/\"\"\"+str(periods)+\"\"\"*100, 2) as percentage,\n",
        "  round(periods/\"\"\"+str(periods)+\"*\"+str(hours)+\"\"\", 2) as hours,\n",
        "  round(periods/\"\"\"+str(periods)+\"*\"+str(hours)+\"*bucket*\"+str(se_price)+\"\"\", 2) as cost_usd_se,\n",
        "  round(periods/\"\"\"+str(periods)+\"*\"+str(hours)+\"*bucket*\"+str(ee_price)+\"\"\", 2) as cost_usd_ee,\n",
        "  round(periods/\"\"\"+str(periods)+\"*\"+str(hours)+\"*bucket*\"+str(eep_price)+\"\"\", 2) as cost_usd_eep\n",
        "FROM\n",
        "  `\"\"\"+project_id+\".\"+dataset+\"\"\".buckets_count`\n",
        "  order by bucket\"\"\"\n",
        "\n",
        "if verbose:\n",
        "  print(query)\n",
        "\n",
        "df = pandas_gbq.read_gbq(query, project_id=project_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lptkZlWMFH1o"
      },
      "source": [
        "**Let's see how it looks like.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fM6iGYiV6qdf"
      },
      "outputs": [],
      "source": [
        "query = \"select * from \"+dataset+\".calculated\"\n",
        "calculated = pandas_gbq.read_gbq(query, project_id=project_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pX8mhC-riKaD"
      },
      "outputs": [],
      "source": [
        "print(calculated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zy8nUuJ9FOSq"
      },
      "source": [
        "**Stage 12**<br>\n",
        "Here is a visualization of the histogram that shows the distribution of the slot buckets and the time spent in each of them.<br>\n",
        "There should be a \"sweet spot\" where most of the buckets under it are heavily used and above it there is only slight usage.<br>\n",
        "This shhould give you the sense of where that sweet spot should be."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNn5E1mT0JWp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "calculated.plot(x=\"bucket\", y=\"percentage\", kind=\"line\",figsize=(15,9))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XF31mB5xZVL4"
      },
      "source": [
        "**Step 13**\n",
        "\n",
        "The following chart shows how many queries ran on each time during our test period."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ph39S02LQQlO"
      },
      "outputs": [],
      "source": [
        "query = \"select period_start, total_queries from \"+dataset+\".slots_full order by period_start\"\n",
        "df = pandas_gbq.read_gbq(query, project_id=project_id)\n",
        "df.plot(x=\"period_start\", y=\"total_queries\", kind=\"line\",figsize=(25,9))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJgFM9h75vZw"
      },
      "source": [
        "**Step 14**<br>\n",
        "Use the following query to identify where slot usage drops below 1% of the time.We will set this bucket as the max_slots as we can probably live with less than 1% of the queries that will get slower."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RCxMi4tZyfF"
      },
      "outputs": [],
      "source": [
        "query = \"select min(bucket) as recommended from \"+dataset+\".calculated where percentage < 1 and bucket > 0\"\n",
        "\n",
        "if verbose:\n",
        "  print(query)\n",
        "\n",
        "df = pandas_gbq.read_gbq(query, project_id=project_id)\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBwS6v6YG2AJ"
      },
      "source": [
        "**Step 15**<br>\n",
        "The following steps try to find the optimal max_slots that best balances cost and performance.<br>\n",
        "As a first step choose the max_slots value you want to check."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-ZFZDCoZhEV"
      },
      "outputs": [],
      "source": [
        "max_slots = 100 # @param {\"type\":\"number\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-fmTKGtghxe"
      },
      "source": [
        "**Explanation**<br>\n",
        "The trade off is cost vs. performance. If we choose the right max_slots than we can reduce the cost while only a small number of queries will decrease in performance. If these are not time critical queries (such as ETLs, background jobs etc.) than we may want to \"sacrifice\" them in return for  lower cost."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ht67chWWgYJU"
      },
      "source": [
        "**Step 16**<br>\n",
        "We try to calculate how much will we pay if we choose the above max_slots.<br>\n",
        "We also add the \"tail\" which are all the buckets that were used less than 1% of the time and now will all accumulate at the max_slots bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cd9arA-rYYfL"
      },
      "outputs": [],
      "source": [
        "answer = calculated[calculated['bucket'] <= max_slots]\n",
        "se = answer['cost_usd_se'].sum()\n",
        "ee = answer['cost_usd_ee'].sum()\n",
        "eep = answer['cost_usd_eep'].sum()\n",
        "\n",
        "query = \"select cost_usd from \"+dataset+\".bytes\"\n",
        "df = pandas_gbq.read_gbq(query, project_id=project_id)\n",
        "cost_bytes = str(round(df[\"cost_usd\"][0]))\n",
        "\n",
        "# Now adding all the percent fragments that were used less than 1% and now will be added to the max_slots bucket\n",
        "tail = calculated[calculated['percentage'] < 1]\n",
        "tail_cost_se = tail['percentage'].sum() * se_price * max_slots\n",
        "tail_cost_ee = tail['percentage'].sum() * ee_price * max_slots\n",
        "tail_cost_eep = tail['percentage'].sum() * eep_price * max_slots\n",
        "\n",
        "print('Total cost estimation for the time frame using Standard edition and maximum slots '+str(max_slots)+' is '+str(round(se + tail_cost_se))+' USD.' )\n",
        "print('Total cost estimation for the time frame using Enterprise edition and maximum slots '+str(max_slots)+' is '+str(round(ee + tail_cost_ee))+' USD.' )\n",
        "print('Total cost estimation for the time frame using Enterprise plus eition and maximum slots '+str(max_slots)+' is '+str(round(eep + tail_cost_eep))+' USD.' )\n",
        "print('Running the same workload with on-demand pricing costed you '+cost_bytes+\" USD\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VlJVjPdiKLO"
      },
      "source": [
        "**Step 17**<br>\n",
        "We want to see which queries will be the most affected by the slot decrease (the ones that has slot consumption above max_slots).\n",
        "Many times you find out that those queries are not time sensitive (such as background processes) and you can sacrifice their performance to lower cost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqDLbWQ4x_dV"
      },
      "outputs": [],
      "source": [
        "query = \"\"\"SELECT\n",
        "  *\n",
        "FROM (\n",
        "  SELECT\n",
        "    timeline.job_id AS job_id,\n",
        "    jobs.query AS query,\n",
        "    jobs.job_type AS job_type,\n",
        "    ROUND(MAX(timeline.period_slot_ms/1000)) AS total_slot_ms,\n",
        "    COUNT(timeline.job_id) AS slices\n",
        "  FROM\n",
        "    `\"\"\"+project_id+\"`.`region-\"+region+\"\"\"`.INFORMATION_SCHEMA.JOBS_TIMELINE AS timeline\n",
        "  JOIN\n",
        "    `\"\"\"+project_id+\"`.`region-\"+region+\"\"\"`.INFORMATION_SCHEMA.JOBS AS jobs\n",
        "  ON\n",
        "    timeline.job_id = jobs.job_id\n",
        "  WHERE\n",
        "    period_start BETWEEN CAST(\\\"\"\"\"+start_timestamp+\"\"\"\\\" AS TIMESTAMP)\n",
        "  AND \"\"\"+\"CAST(\\\"\"+end_timestamp+\"\\\" AS TIMESTAMP)\"\"\"+\"\"\"\n",
        "  GROUP BY\n",
        "    job_id,\n",
        "    query,\n",
        "    job_type\n",
        "  ORDER BY\n",
        "    slices DESC)\n",
        "WHERE\n",
        "  total_slot_ms>\"\"\"+str(max_slots)\n",
        "\n",
        "if verbose:\n",
        "  print(query)\n",
        "\n",
        "df = pandas_gbq.read_gbq(query, project_id=project_id)\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDL0sCpBdg7H"
      },
      "source": [
        "**Step 18**\n",
        "\n",
        "Here we calculate what is the optimal baseline slots value, if any."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbgVvPKFdlf4"
      },
      "outputs": [],
      "source": [
        "query = \"\"\"SELECT\n",
        "  *\n",
        "FROM\n",
        "  \"\"\"+dataset+\"\"\".calculated\n",
        "WHERE\n",
        "  percentage>60\n",
        "  and bucket>0\"\"\"\n",
        "\n",
        "if verbose:\n",
        "  print(query)\n",
        "\n",
        "df = pandas_gbq.read_gbq(query, project_id=project_id)\n",
        "if df.empty:\n",
        "  print('It is recommended not to set a baseline.')\n",
        "else:\n",
        "  bucket = df['bucket'][0]\n",
        "  cost_se = df['cost_usd_se'][0]\n",
        "  cost_ee = df['cost_usd_ee'][0]\n",
        "  cost_eep = df['cost_usd_eep'][0]\n",
        "  percentage = df['percentage'][0]\n",
        "  if percentage > 60 and percentage < 80 :\n",
        "    print(\"Setting a baseline of \"+str(bucket)+\"\"\" with 3 Year commitment will save you this much for the tested period:\n",
        "     With Standard edition: \"\"\"+str(cost_se*0.4)+\"\"\" dollars\n",
        "     With Enterprise edition: \"\"\"+str(cost_ee*0.4)+\"\"\" dollars\n",
        "     With Enterprise plus edition: \"\"\"+str(cost_eep*0.4)+\" dollars\")\n",
        "  if percentage >= 80 :\n",
        "    print(\"Setting a baseline of \"+str(bucket)+\"\"\" with 1 Year commitment will save you this much for the tested period:\n",
        "     With Standard edition: \"\"\"+str(round(cost_se*0.2))+\"\"\" dollars\n",
        "     With Enterprise edition: \"\"\"+str(round(cost_ee*0.2))+\"\"\" dollars\n",
        "     With Enterprise plus edition: \"\"\"+str(round(cost_eep*0.2))+\" dollars\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HgO2RR1Of_o"
      },
      "source": [
        "**Step 19**\n",
        "\n",
        "Here we create a chart that shows how many queries ran during our test period. We also break them down to background queries that were scheduled or triggered by automatic tools such as cloud scheduler, Composer etc (and hence are less time sensitive) and foreground queries that were run interactively by human users."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7dRpd-eKBWf"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "query = \"SELECT hour, fg_count, bg_count FROM \"+dataset+\".query_types  ORDER BY hour\"\n",
        "if verbose:\n",
        "  print(query)\n",
        "\n",
        "df = pandas_gbq.read_gbq(query, project_id=project_id)\n",
        "\n",
        "# Plotting the stacked bar chart\n",
        "fig, ax = plt.subplots(figsize=(33,7))\n",
        "\n",
        "ax.bar(df['hour'], df['bg_count'], label='Background', width = 0.03)\n",
        "ax.bar(df['hour'], df[\"fg_count\"], bottom=df['bg_count'], label='Foreground', width = 0.03)\n",
        "\n",
        "# Adding labels and title\n",
        "ax.set_xlabel('Hour')\n",
        "ax.set_ylabel('Queries')\n",
        "ax.set_title('Breakdown of query types per hour')\n",
        "ax.legend()\n",
        "\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n",
        "cnt_bg = df[\"bg_count\"].sum()\n",
        "cnt_fg = df[\"fg_count\"].sum()\n",
        "total = cnt_fg + cnt_bg\n",
        "print (str(round(cnt_bg/(total/100),2))+\" percent of the queries were background queries and \"+str(round(cnt_fg/(total/100),2))+ \" were foreground queries\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ck__I1gknhno"
      },
      "source": [
        "**Step 20**\n",
        "\n",
        "Here we do pretty much the same as we did in the previous step, only instead of grouping the queries into foreground/background we divide by the job types (query, load, extract and null).\n",
        "\n",
        "Null is used for internal queries like materialized vies refresh."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erbgeVWsMcW_"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "query = \"SELECT hour, query,load,extract_,null_ FROM \"+dataset+\".job_types  ORDER BY hour\"\n",
        "if verbose:\n",
        "  print(query)\n",
        "\n",
        "df = pandas_gbq.read_gbq(query, project_id=project_id)\n",
        "\n",
        "# Plotting the stacked bar chart\n",
        "fig, ax = plt.subplots(figsize=(33,7))\n",
        "\n",
        "ax.bar(df['hour'], df['query'], label='Query', width = 0.03)\n",
        "ax.bar(df['hour'], df[\"load\"], bottom=df['query'], label='Load', width = 0.03)\n",
        "ax.bar(df['hour'], df[\"extract_\"], bottom=df['load'], label='Extract', width = 0.03)\n",
        "ax.bar(df['hour'], df[\"null_\"], bottom=df['extract_'], label='Null', width = 0.03)\n",
        "\n",
        "# Adding labels and title\n",
        "ax.set_xlabel('Hour')\n",
        "ax.set_ylabel('Queries')\n",
        "ax.set_title('Breakdown of job types per hour')\n",
        "ax.legend()\n",
        "\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n",
        "cnt_query = df[\"query\"].sum()\n",
        "cnt_load = df[\"load\"].sum()\n",
        "cnt_extract = df[\"extract_\"].sum()\n",
        "cnt_null = df[\"null_\"].sum()\n",
        "\n",
        "total = cnt_query+cnt_load+cnt_extract+cnt_null\n",
        "\n",
        "print (str(round(cnt_query/(total/100),2))+\" percent of the jobs were query jobs, \"+str(round(cnt_load/(total/100),2))+ \" were load jobs,\")\n",
        "print (str(round(cnt_extract/(total/100),2))+\" percent of the jobs were extract jobs and \"+str(round(cnt_null/(total/100),2))+ \" were null (internal) jobs.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVeUQWLFi9oh"
      },
      "source": [
        "**Step 21**<br>\n",
        "Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C83CPECgi8QH"
      },
      "outputs": [],
      "source": [
        "queries = []\n",
        "queries.append(\"drop view \"+dataset+\".calculated\")\n",
        "queries.append(\"drop view \"+dataset+\".buckets_count\")\n",
        "queries.append(\"drop view \"+dataset+\".bucketed\")\n",
        "queries.append(\"drop table \"+dataset+\".bytes\")\n",
        "queries.append(\"drop table \"+dataset+\".slots\")\n",
        "queries.append(\"drop table \"+dataset+\".job_types\")\n",
        "queries.append(\"drop table \"+dataset+\".query_types\")\n",
        "queries.append(\"drop table \"+dataset+\".jobs\")\n",
        "queries.append(\"drop view \"+dataset+\".slots_full\")\n",
        "queries.append(\"drop table \"+dataset+\".silent_periods\")\n",
        "\n",
        "for query in queries:\n",
        "  df = pandas_gbq.read_gbq(query, project_id=project_id)\n",
        "\n",
        "print (\"Delete complete.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "cell_execution_strategy": "setup",
      "name": "Slot_machine",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}