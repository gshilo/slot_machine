{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM7NCS6mPpU6"
      },
      "source": [
        "\n",
        "# **Slot Machine**<br>\n",
        "If you are using BigQuery with the on demand billing model where you are billed by bytes scanned, it may be a good idea to switch to the Editions model where you are billed for slots you used.\n",
        "\n",
        "There are Two quite complicated querstions though:\n",
        "\n",
        "\n",
        "1.   Will moving to Editions save me money ?\n",
        "2.   How many slots should I reserve, what is the optimal max_xlots value ?\n",
        "\n",
        "There is no way to be 100% accurate, but this set of queries aims to reduce the guesswork and guide you in finding better answers to the Two questions above.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wOvhZiARoLC"
      },
      "source": [
        "**Usage**<br>\n",
        "You should run this in the project you want to test.\n",
        "The process creates several tables and views, if any of them already exists then it will stop with error, since we don't want to accidently delete user's important tables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_LhcAbq4NCG"
      },
      "source": [
        "**Step 1**<br>\n",
        "First we set some parameters such as region, timeframe and BigQuery edition. We also detect the current project id.\n",
        "\n",
        "Here is a short description of the parameters:\n",
        "\n",
        "- **region**: The region where your dataset is located. INFORMATION_SCHEMA is a regional data source.\n",
        "- **dataset**: the dataset where you want the tables and views related to slot machine to be created. It's a good idea to create a dedicated dataset so we won't accidently overwrite a production table or view.\n",
        "- **start and end timestamp**: We analyze the behavior in a limited timeframe. It should be long enough to contain all regular activity so it should contain at lease few days.\n",
        "- **edition**: BigQuery compute has Three editions when using reservations which are priced differently. Choosing the edition affects total price and available features. You can read more [here](https://cloud.google.com/bigquery/docs/editions-intro).\n",
        "- **verbose**: If true, Slot machine will print out the queries it runs plus additional interim data. If false it will operate quietly without printing out every query and interim result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQumnhKuKTDy"
      },
      "outputs": [],
      "source": [
        "import google.auth\n",
        "project_id = google.auth.default()[1]\n",
        "region = \"US\" # @param {\"type\":\"string\", \"placeholder\": \"Enter dataset region\"}\n",
        "dataset = \"test2\" # @param {\"type\":\"string\", \"placeholder\": \"Enter the target dataset where objects will be created\"}\n",
        "start_timestamp = \"2024-09-23\" # @param {\"type\":\"date\"}\n",
        "end_timestamp = \"2024-09-30\" # @param {\"type\":\"date\"}\n",
        "edition = \"Standard\" # @param [\"Standard\", \"Enterprise\", \"Enterprise plus\"]\n",
        "verbose = True # @param {\"type\":\"boolean\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9McWPU9c4gC2"
      },
      "source": [
        "**Step 2**<br>\n",
        "Here we import some python packages and create the dataset if it does not already exist."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HEnFFD1o7CJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from google.cloud.exceptions import NotFound\n",
        "\n",
        "client = bigquery.Client()\n",
        "my_dataset = bigquery.Dataset(project_id+\".\"+dataset)\n",
        "my_dataset.location = region\n",
        "\n",
        "\n",
        "try:\n",
        "    client.get_dataset(project_id+\".\"+dataset)\n",
        "    print(\"Dataset {} already exists\".format(dataset))\n",
        "except NotFound:\n",
        "    print(\"Dataset {} is not found\".format(dataset), \"creating it.\")\n",
        "    dataset_object = client.create_dataset(my_dataset, timeout=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGfdPMkHGSA2"
      },
      "source": [
        "**Step 3**<br>\n",
        "Ceate a table with the on demand consumption for reference. The table name is bytes.<br>\n",
        "We select from INFORMATION_SCHEMA.JOBS to find the total bytes billed during the timeframe and what was the cost in USD.<br>Write down the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtC29YYAvYJK"
      },
      "outputs": [],
      "source": [
        "print(dataset)\n",
        "query = \"create table \"+dataset+\".bytes as \"+\"\"\"SELECT\n",
        "  SUM(total_bytes_billed/1024/1024/1024/1024) AS total_tb_billed,\n",
        "  SUM(total_bytes_billed/1024/1024/1024/1024)*6.25 as cost_usd\n",
        "FROM \"\"\"+project_id+\".region-\"+region+\"\"\".INFORMATION_SCHEMA.JOBS\n",
        "WHERE\n",
        "  creation_time BETWEEN CAST(\\\"\"\"\"+start_timestamp+\"\"\"\\\" AS TIMESTAMP)\n",
        "  AND \"\"\"+\"CAST(\\\"\"+end_timestamp+\"\\\" AS TIMESTAMP)\"\n",
        "\n",
        "if verbose:\n",
        "  print(query)\n",
        "\n",
        "df = pd.io.gbq.read_gbq(query=query, project_id=project_id, dialect='standard')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eg60a_RPoR6b"
      },
      "source": [
        "**Step 4**<br>\n",
        "Then we query INFORMATION_SCHEMA.JOBS_TIMELINE on the same time range to see how many slots were used in every second of the time range. We send the result to a table called slots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKCiC-HYdD0d"
      },
      "outputs": [],
      "source": [
        "query = \"create table \"+dataset+\".slots as \"+\"\"\"SELECT\n",
        "  period_start,\n",
        "  SUM(period_slot_ms/1000) AS total_slot_ms,\n",
        "FROM\n",
        "  \"\"\"+project_id+\".region-\"+region+\"\"\".INFORMATION_SCHEMA.JOBS_TIMELINE\n",
        "WHERE\n",
        "  period_start BETWEEN CAST(\\\"\"\"\"+start_timestamp+\"\"\"\\\" AS TIMESTAMP)\n",
        "  AND \"\"\"+\"CAST(\\\"\"+end_timestamp+\"\\\" AS TIMESTAMP)\"\"\"\n",
        "\n",
        "query = query + \"\"\"\n",
        "GROUP BY\n",
        "  period_start\n",
        "ORDER BY\n",
        "  period_start DESC\"\"\"\n",
        "\n",
        "if verbose:\n",
        "  print(query)\n",
        "df = pd.io.gbq.read_gbq(query=query, project_id=project_id, dialect='standard')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5**\n",
        "\n",
        "Create jobs table that later will be used to chart the number of jobs that ran in each time period along with the foregraoud/background jobs breakdown."
      ],
      "metadata": {
        "id": "qm_kRsP5FOxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"create table \"+dataset+\"\"\".jobs as SELECT\n",
        "  timeline.job_id,\n",
        "  timestamp_trunc(timeline.period_start, MINUTE) period_start,\n",
        "  jobs.user_email,\n",
        "  jobs.Job_type,\n",
        "  CASE\n",
        "    WHEN CONTAINS_SUBSTR(jobs.user_email, 'gserviceaccount') THEN 'background'\n",
        "    ELSE 'foreground'\n",
        "END\n",
        "  AS query_type\n",
        "FROM\n",
        "    `\"\"\"+project_id+\".region-\"+region+\"\"\".INFORMATION_SCHEMA.JOBS_TIMELINE` AS timeline\n",
        "  JOIN\n",
        "    `\"\"\"+project_id+\".region-\"+region+\"\"\".INFORMATION_SCHEMA.JOBS` AS jobs\n",
        "  ON\n",
        "    timeline.job_id = jobs.job_id\n",
        "WHERE\n",
        "  timeline.period_start BETWEEN CAST(\"\"\"+\"\\\"\"+start_timestamp+\"\\\"\"+\"\"\" AS TIMESTAMP)\n",
        "  AND CAST(\"\"\"+\"\\\"\"+end_timestamp+\"\\\"\"+\" AS TIMESTAMP)\"\n",
        "\n",
        "if verbose:\n",
        "  print(query)\n",
        "df = pd.io.gbq.read_gbq(query=query, project_id=project_id, dialect='standard')"
      ],
      "metadata": {
        "id": "hlxh9_CTE6JZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyK-sFMX5sPj"
      },
      "source": [
        "**Step 5**<br>\n",
        "We create a dynamic query that creates buckets incremented by 50 from 0 to the highest number of slots used during the selected time period.<br>\n",
        "The original time periods are seconds, so we aggregate the data in Minute granularity and for each minute we take the high<br> watermark of slot usage (since BigQuery autoscaler's minimum is 1 minute).<br><br>\n",
        "Then we assign each minute to a specific bucket according to the maximum slots that ut used.<br>\n",
        "We create a view called bucketed to hold the result and enable further calculations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hR7o0v4Hd0f3"
      },
      "outputs": [],
      "source": [
        "query1 = \"select max(total_slot_ms) as max_slots from \"+dataset+\".slots\"\n",
        "df = pd.io.gbq.read_gbq(query=query1, project_id=project_id, dialect='standard')\n",
        "max_slots = int(df._get_value(0, 'max_slots'))\n",
        "query2 = \"create view \"+dataset+\".bucketed\"+\"\"\" as SELECT\n",
        "  timestamp_trunc(period_start, MINUTE) as period_start,\n",
        "  max(case\n",
        "when total_slot_ms = 0 then 0\\n\"\"\"\n",
        "for i in range(0, max_slots, 50):\n",
        "  line = \"when total_slot_ms between \"+str(i)+\" and \"+str(i+50)+\" then \"+str(i+50)+'\\n'\n",
        "  query2 = query2 + line\n",
        "query2 = query2 + \"\"\"  end) as bucket\n",
        "FROM \"\"\"+dataset+\"\"\".slots\n",
        "  group by timestamp_trunc(period_start, MINUTE)\"\"\"\n",
        "\n",
        "if verbose:\n",
        "  print(query2)\n",
        "\n",
        "df = pd.io.gbq.read_gbq(query=query2, project_id=project_id, dialect='standard')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hB8ZVTWwAV7g"
      },
      "source": [
        "**Step 6**<br>\n",
        "Here we create a view called buckets_count that shows hoe many time periods (minutes) fall into each bucket.<br>\n",
        "Then we print the contents of the view."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvRkZok7eHMx"
      },
      "outputs": [],
      "source": [
        "query = \"create view \"+dataset+\"\"\".buckets_count as SELECT\n",
        "  bucket,\n",
        "  COUNT(*) as periods,\n",
        "FROM `\"\"\"+project_id+\".\"+dataset+\"\"\".bucketed`\n",
        "where bucket is not null\n",
        "GROUP BY\n",
        "  bucket\"\"\"\n",
        "\n",
        "if verbose:\n",
        "  print(query)\n",
        "\n",
        "df = pd.io.gbq.read_gbq(query=query, project_id=project_id, dialect='standard')\n",
        "query = 'select * from '+dataset+\".buckets_count order by bucket\"\n",
        "\n",
        "if verbose:\n",
        "  print(query)\n",
        "df = pd.io.gbq.read_gbq(query=query, project_id=project_id, dialect='standard')\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdAz0WMoChFM"
      },
      "source": [
        "For our next calculations we need to know Two numbers:\n",
        "\n",
        "\n",
        "1.   How many hours there are in our timeframe\n",
        "2.   How many time periods we have\n",
        "\n",
        "So steps 7 and 8 find those values.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uTDf2dtBhq0"
      },
      "source": [
        "**Step 7**<br>\n",
        "Calculate how many hours we have in the chosen time frame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znsJmnaRi_OS"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "date_format = '%Y-%m-%d'\n",
        "diff = datetime.strptime(end_timestamp,date_format) - datetime.strptime(start_timestamp, date_format)\n",
        "hours = diff.days * 24 + diff.seconds / 3600\n",
        "if verbose:\n",
        "  print(\"hours: \"+str(hours))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68WBq6DnDAGK"
      },
      "source": [
        "**Step 8**<br>\n",
        "Calculate how many time slots we have."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4UOfEKHC1Ha"
      },
      "outputs": [],
      "source": [
        "query = \"select count(*) as periods from \"+dataset+\".bucketed\"\n",
        "df = pd.io.gbq.read_gbq(query=query, project_id=project_id, dialect='standard')\n",
        "periods = df._get_value(0, 'periods')\n",
        "if verbose:\n",
        "  print(\"periods: \"+str(periods))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IerxqG84D_ji"
      },
      "source": [
        "**Step 9**<br>\n",
        "This is the heart of our calculation.<br><br>\n",
        "We know how many hours were in the selected timeframe and how many time periods we had during this timeframe.\n",
        "So we can calculate what is the percentage of all the time that we used each bucket.<br>\n",
        "If we know the percentage of time and we know how many wours we had in the timeframe then we can calculate how many hours each bucket was active.<br>\n",
        "And we know how much each slot/hour costs for each BigQuery edition so we can calculate the cost each bucket incured.\n",
        "\n",
        "So here we create a view called calculated that holds the bucket, the percentage of all time that this bucket was used, how many hours it was used, and how much we would be charged for it if we use the selected edition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpMDkjb3bqIo"
      },
      "outputs": [],
      "source": [
        "if edition == 'Enterprise':\n",
        "  price = 0.06\n",
        "elif edition == 'Enterprise plus':\n",
        "  price = 0.1\n",
        "else:\n",
        "  price = 0.04\n",
        "\n",
        "query = \"create view \"+dataset+\"\"\".calculated as SELECT\n",
        "  bucket,\n",
        "  round(periods/\"\"\"+str(periods)+\"\"\"*100, 3) as percentage,\n",
        "  round(periods/\"\"\"+str(periods)+\"*100/100*\"+str(hours)+\"\"\", 3) as hours,\n",
        "  round(periods/\"\"\"+str(periods)+\"*100/100*\"+str(hours)+\"*bucket*\"+str(price)+\"\"\", 3) as cost_usd\n",
        "FROM\n",
        "  `\"\"\"+project_id+\".\"+dataset+\"\"\".buckets_count`\n",
        "  order by bucket\"\"\"\n",
        "\n",
        "if verbose:\n",
        "  print(query)\n",
        "\n",
        "df = pd.io.gbq.read_gbq(query=query, project_id=project_id, dialect='standard')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lptkZlWMFH1o"
      },
      "source": [
        "**Let's see how it looks like.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fM6iGYiV6qdf"
      },
      "outputs": [],
      "source": [
        "query = \"select * from \"+dataset+\".calculated\"\n",
        "calculated = pd.io.gbq.read_gbq(query=query, project_id=project_id, dialect='standard')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pX8mhC-riKaD"
      },
      "outputs": [],
      "source": [
        "print(calculated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zy8nUuJ9FOSq"
      },
      "source": [
        "**Stage 10**<br>\n",
        "Here is a visualization of the histogram that shows the distribution of the slot buckets and the time spent in each of them.<br>\n",
        "There should be a \"sweet spot\" where most of the buckets under it are heavily used and above it there is only slight usage.<br>\n",
        "This shhould give you the sense of where that sweet spot should be."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNn5E1mT0JWp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "calculated.plot(x=\"bucket\", y=\"percentage\", kind=\"line\",figsize=(15,9))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"select period_start, total_queries from \"+dataset+\".slots order by period_start\"\n",
        "df = pd.io.gbq.read_gbq(query=query, project_id=project_id, dialect='standard')\n",
        "df.plot(x=\"period_start\", y=\"total_queries\", kind=\"line\",figsize=(15,9))\n",
        "#ts = pd.Series(df[total_queries], index=pd.period_start)\n",
        "\n",
        "#ts = ts.cumsum()\n",
        "#ts.plot()"
      ],
      "metadata": {
        "id": "ph39S02LQQlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJgFM9h75vZw"
      },
      "source": [
        "**Step 11**<br>\n",
        "Use the following query to identify where slot usage drops below 1% of the time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RCxMi4tZyfF"
      },
      "outputs": [],
      "source": [
        "query = \"select max(bucket) as recommended from \"+dataset+\".calculated where percentage > 1\"\n",
        "\n",
        "if verbose:\n",
        "  print(query)\n",
        "\n",
        "df = pd.io.gbq.read_gbq(query=query, project_id=project_id, dialect='standard')\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBwS6v6YG2AJ"
      },
      "source": [
        "**Step 12**<br>\n",
        "The following steps try to find the optimal max_slots that best balances cost and performance.<br>\n",
        "As a first step choose the max_slots value you want to check."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-ZFZDCoZhEV"
      },
      "outputs": [],
      "source": [
        "max_slots = 750 # @param {\"type\":\"number\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-fmTKGtghxe"
      },
      "source": [
        "**Explanation**<br>\n",
        "The trade off is cost vs. performance. If we choose the right max_slots than we can reduce the cost while only a small number of queries will decrease in performance. If these are not time critical queries (such as ETLs, background jobs etc.) than we may want to \"sacrifice\" them in return for  lower cost."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ht67chWWgYJU"
      },
      "source": [
        "**Step 13**<br>\n",
        "We try to calculate how much will we pay if we choose the above max_slots.<br>\n",
        "This is a naive approach as it does not take into consideration all the buckers above the max_slots, as they all had 0 percent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cd9arA-rYYfL"
      },
      "outputs": [],
      "source": [
        "answer = calculated[calculated['bucket'] <= max_slots]\n",
        "summed = answer['cost_usd'].sum()\n",
        "print('Total cost estimation for the time frame using maximum slots '+str(max_slots)+' is '+str(summed)+' USD.' )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VlJVjPdiKLO"
      },
      "source": [
        "**Step 14**<br>\n",
        "Finally, we want to see which queries will be the most affected by the slot decrease (the ones that has slot consumption above max_slots).\n",
        "Many times you find out that those queries are not time sensitive (such as background processes) and you can sacrifice their performance to lower cost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqDLbWQ4x_dV"
      },
      "outputs": [],
      "source": [
        "query = \"\"\"SELECT\n",
        "  *\n",
        "FROM (\n",
        "  SELECT\n",
        "    timeline.job_id AS job_id,\n",
        "    jobs.query AS query,\n",
        "    jobs.job_type AS job_type,\n",
        "    ROUND(MAX(timeline.period_slot_ms/1000)) AS total_slot_ms,\n",
        "    COUNT(timeline.job_id) AS slices\n",
        "  FROM\n",
        "    `\"\"\"+project_id+\"`.`region-\"+region+\"\"\"`.INFORMATION_SCHEMA.JOBS_TIMELINE AS timeline\n",
        "  JOIN\n",
        "    `\"\"\"+project_id+\"`.`region-\"+region+\"\"\"`.INFORMATION_SCHEMA.JOBS AS jobs\n",
        "  ON\n",
        "    timeline.job_id = jobs.job_id\n",
        "  WHERE\n",
        "    period_start BETWEEN CAST(\\\"\"\"\"+start_timestamp+\"\"\"\\\" AS TIMESTAMP)\n",
        "  AND \"\"\"+\"CAST(\\\"\"+end_timestamp+\"\\\" AS TIMESTAMP)\"\"\"+\"\"\"\n",
        "  GROUP BY\n",
        "    job_id,\n",
        "    query,\n",
        "    job_type\n",
        "  ORDER BY\n",
        "    slices DESC)\n",
        "WHERE\n",
        "  total_slot_ms>\"\"\"+str(max_slots)\n",
        "\n",
        "if verbose:\n",
        "  print(query)\n",
        "\n",
        "df = pd.io.gbq.read_gbq(query=query, project_id=project_id, dialect='standard')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 15**\n",
        "\n",
        "Here we calculate what is the optimal baseline slots value, if any."
      ],
      "metadata": {
        "id": "xDL0sCpBdg7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"\"\"SELECT\n",
        "  bucket,\n",
        "  cost_usd,\n",
        "  MAX(percentage) AS percentage\n",
        "FROM\n",
        "  \"\"\"+dataset+\"\"\".calculated\n",
        "WHERE\n",
        "  percentage>60\n",
        "GROUP BY\n",
        "  bucket, cost_usd\"\"\"\n",
        "\n",
        "if verbose:\n",
        "  print(query)\n",
        "\n",
        "df = pd.io.gbq.read_gbq(query=query, project_id=project_id, dialect='standard')\n",
        "if df.empty:\n",
        "  print('It is recommended not to set a baseline.')\n",
        "else:\n",
        "  bucket = df['bucket'][0]\n",
        "  cost = df['cost_usd'][0]\n",
        "  percentage = df['percentage'][0]\n",
        "  if percentage>60 :\n",
        "    print(\"Setting a baseline of \"+str(bucket)+\" with 3 Year commitment will save you \"+str(cost*0.4)+\" dollars for the tested period.\")\n",
        "  if percentage>80 :\n",
        "    print(\"Setting a baseline of \"+str(bucket)+\" with 1 Year commitment will save you \"+str(cost*0.2)+\" dollars for the tested period.\")"
      ],
      "metadata": {
        "id": "jbgVvPKFdlf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 16**\n",
        "\n",
        "Here we check how many queries were run during our test period. We also break them down to background queries that were scheduled or triggered by automatic tools such as cloud scheduler, Composer etc (and hence are less time sensitive) and foreground queries that were run interactively by human users."
      ],
      "metadata": {
        "id": "1HgO2RR1Of_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"CREATE OR REPLACE VIEW \"+dataset+\"\"\".jobs_count AS\n",
        "SELECT\n",
        "  TIMESTAMP_TRUNC(period_start, HOUR) AS hour,\n",
        "  SUM(foreground) AS fg_count,\n",
        "  SUM(background) AS bg_count\n",
        "FROM (\n",
        "  SELECT\n",
        "    period_start,\n",
        "    CASE query_type\n",
        "      WHEN 'foreground' THEN 1\n",
        "      ELSE 0\n",
        "  END\n",
        "    AS foreground,\n",
        "    CASE query_type\n",
        "      WHEN 'background' THEN 1\n",
        "      ELSE 0\n",
        "  END\n",
        "    AS background\n",
        "  FROM\n",
        "    \"\"\"+dataset+\"\"\".jobs)\n",
        "GROUP BY\n",
        "  hour\n",
        "ORDER BY\n",
        "  hour\"\"\"\n",
        "\n",
        "if verbose:\n",
        "  print(query)\n",
        "\n",
        "df = pd.io.gbq.read_gbq(query=query, project_id=project_id, dialect='standard')\n"
      ],
      "metadata": {
        "id": "QWYjkYTMFv3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "query = \"SELECT hour, fg_count, bg_count FROM \"+dataset+\".jobs_count  ORDER BY hour\"\n",
        "if verbose:\n",
        "  print(query)\n",
        "\n",
        "df = pd.io.gbq.read_gbq(query=query, project_id=project_id, dialect='standard')\n",
        "\n",
        "# Plotting the stacked bar chart\n",
        "fig, ax = plt.subplots(figsize=(33,7))\n",
        "\n",
        "ax.bar(df['hour'], df['bg_count'], label='Background', width = 0.03)\n",
        "ax.bar(df['hour'], df[\"fg_count\"], bottom=df['bg_count'], label='Foreground', width = 0.03)\n",
        "\n",
        "# Adding labels and title\n",
        "ax.set_xlabel('Hour')\n",
        "ax.set_ylabel('Queries')\n",
        "ax.set_title('Query count per hour')\n",
        "ax.legend()\n",
        "\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n",
        "cnt_bg = df[\"bg_count\"].sum()\n",
        "cnt_fg = df[\"fg_count\"].sum()\n",
        "total = cnt_fg + cnt_bg\n",
        "print (str(round(cnt_bg/(total/100),2))+\" percent of the queries were background queries and \"+str(round(cnt_fg/(total/100),2))+ \" were foreground queries\")\n"
      ],
      "metadata": {
        "id": "f7dRpd-eKBWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVeUQWLFi9oh"
      },
      "source": [
        "**Step 17**<br>\n",
        "Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C83CPECgi8QH"
      },
      "outputs": [],
      "source": [
        "queries = []\n",
        "queries.append(\"drop view \"+dataset+\".calculated\")\n",
        "queries.append(\"drop view \"+dataset+\".buckets_count\")\n",
        "queries.append(\"drop view \"+dataset+\".bucketed\")\n",
        "queries.append(\"drop table \"+dataset+\".bytes\")\n",
        "queries.append(\"drop table \"+dataset+\".slots\")\n",
        "\n",
        "for query in queries:\n",
        "  df = pd.io.gbq.read_gbq(query=query, project_id=project_id, dialect='standard')\n",
        "\n",
        "print (\"Delete complete.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "cell_execution_strategy": "setup",
      "name": "Slot_machine",
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}